# -*- coding: utf-8 -*-
"""Notebook final_France.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ooxFB-Ce3Guur3j9qFARve-5bDK_Xl_N

**Indication pour tourner le code** : nous avons codé à partir de google colaboratory. Afin de récupérer les CSV, nous sommes passés par une fonction permettant de les récupérer depuis notre propre drive. 
Pour récupérer un CSV, il faut donc télécharger directement le fichier sur "Mydrive" puis run les ligne "#pathData='/content/drive/MyDrive/Confiance_France.xls'", et "'/content/drive/MyDrive/PIB_France.xlsx'" mises en commentaire.

### I. Indicateurs de la confiance en France  

Dans un premier temps, nous souhaitons analyser les corrélations entre la confiance et les agrégats économiques en France. 
Pour ce faire, nous reprenons la base de l'INSEE "Enquête mensuelle de conjoncture auprès des ménages - mai 2021". Le but est alors d'analyser les rapports entre l'indice de confiance et le PIB, ainsi que les rapports entre l'indice de confiance et la consommation des ménages. 
L'indice de confiance de l'INSEE résume leur opinion sur la situation économique : plus sa valeur est élevée, plus le jugement des ménages sur la situation économique est favorable.
"""

#Importation des packages nécessaires

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns

"""### **La préparation des bases**

Nous importons ensuite la  base de l'INSEE. 
Nous avions choisi "Enquête mensuelle de conjoncture auprès des ménages - mai 2021" car elle nous permet tout d'abord de disposer d'un indice de confiance. Nous n'avons donc pas besoin, grâce à cette base, de créer un nouvel indice. 
Cet indice de confiance est nécessaire : il regroupe chacune des variables, qui sont très corrélées entre elles. 

De plus, cette base nous offre un temps long, de 1972 à 2021. Ceci permet d'avoir un panorama important, avec des situations économiques différentes. Elle comprend en effet les premiers chocs pétroliers, mais aussi des périodes de rebonds économiques, comme la fin des années 1980. 

Finalement, elle regroupe les principales données lorsque l'on considère la confiance en économie : le chômage, le niveau des prix, la capacité d'épargner, et finalement les situations personelles.
"""

#On récupère ainsi la base de données de l'INSEE portant sur la confiance que l'on nomme df


from google.colab import drive
drive.mount('/content/drive')
pathData='/content/drive/MyDrive/Confiance_France.xls'
#pathData='/content/drive/MyDrive/Projet Python économiste/BDD/Confiance_France.xls' #(dépend de l'utilisateur et du chemin pour accéder aux données)
df=pd.read_excel(pathData)

df=df.drop(0)
df

"""Le but final de ce code est de réaliser des régressions linéaires entre les données d'une nouvelle base (base du PIB français et de la consommation des ménages) et les données de cette base de l'INSEE, portant sur la confiance. 
Or, afin de merger les données du PIB (et de la consommation des ménages) sur cette base de l'INSEE, il faut avoir une temporalité similaire. La temporalité de la base du PIB est annuelle. 
De plus, nous souhaitons avoir avant tout des tendances de long terme, ce qui convient donc à un niveau annuel. 
Nous choisisons donc tout d'abord de modifier le format des dates pour ne retenir que l'année, puis de faire une moyenne annuelle pour chaque indicateur (indicateur synthétique de confiance, niveau de vie en France ...) à partir de ces données.
"""

#On remplace les dates par l'année, afin de merger avec la table de données sur le PIB 

df['DATE'] = pd.to_datetime(df['DATE'], infer_datetime_format=True)
df['Année'] = pd.DatetimeIndex(df['DATE']).year
df=df.drop(['DATE'], axis=1)
display(df)

#On reset l'index, et puis on groupe les valeurs par année, en prenant la moyenne des indicateurs sur l'année 
df= df.reset_index(drop=True)
df=df.astype(float)
df=df.groupby(['Année']).mean()
display(df)

"""Pour avoir une idée du type de données qui sont à notre disposition, nous réalisons quelques statistiques descriptives. """

display(df.describe())

"""*Nous avons 50 observations, c'est-à-dire une période de 50 années.*
*En regardant la moyenne des données et leur minimum, on peut s'apercevoir que ces données représentent des niveaux d'évolution. En moyenne, par exemple, le solde d'opinion des ménages sur l'évolution passée de leur niveau de vie a perdu 37 points entre deux années. Au plus bas niveau, le solde d'opinion sur le niveau de vie a diminué de 75 points.*
*Des valeurs négatives indiquent donc une baisse du solde (ou de points pour l'indicateur synthétique), alors que les valeurs positives indiquent une hausse.*

On récupère ensuite la base contenant les données du PIB et la consommation des ménages. 
Cette base de données, de l'INSEE, "Évolution du produit intérieur brut et de ses composantes", nous donne les données françaises exprimées en pourcentage. 
Nous pouvons donc comparer les données de notre première base de données avec cette nouvelle base de données, elle aussi exprimant des évolutions.


Cette base a été choisie simplement pour son caractére descriptif (le PIB + la consommation des ménages). Elle est annuelle.
"""

#On récupère l'excel_PIB pour faire une régression que l'on nomme df2

#df1 = pd.read_excel('/Users/acoss/OneDrive/Bureau/Eco_PIB_France_xls/econ-gen-pib-composante_evolution.xlsx')
from google.colab import drive
drive.mount('/content/drive')

pathData='/content/drive/MyDrive/PIB_France.xlsx'
#pathData='/content/drive/MyDrive/Projet Python économiste/BDD/PIB_France.xlsx'

df1=pd.read_excel(pathData)

"""La base "Évolution du produit intérieur brut et de ses composantes" contient différents agrégats économiques. Il existe également comme donnée "la formation brute de capitale fixe" ou "exportations de biens et services", mais, au vue de notre ancienne base, nous avons décidé de s'arrêter uniquement sur les données en lien avec les ménages français. 
La base est par la suite transposée pour qu'elle puisse être mergée à notre base sur la confiance des ménages
"""

#On enlève les colonnes qui ne nous servent pas
df1 = df1.iloc[[2,4,7]]
df1 = df1.reset_index(drop = True)
#On transpose notre base de données pour qu'elle soit dans le même format que la première base de données, et on reset l'indexe
df1 = df1.T
df1 = df1.reset_index(drop = True)
df1.columns = ['Année' ,'PIB' ,'Consommation']
df1

"""La base est par la suite mise en forme pour que l'axe des années remplace celui de des indices. 
De plus, un (r) succède à 2018 et 2019, nous l'enlevons donc. 
"""

df1['Année'].iloc[-2]=2019.0
df1['Année'].iloc[-3]=2018.0
df1.index = df1['Année']
df1=df1.drop('Année', axis=1)
df1

"""Les deux bases sont donc enfin prêtes pour ne former plus qu'une seule base. 
La concaténation permet donc de n'avoir plus qu'une seule base, et réaliser les régressions linéaires souhaitées. 
"""

#Concaténaton de df1 et df afin de faire des graphiques et la régression

d = [df,df1]
Data = pd.concat(d, join='inner', axis=1)
Data

"""---
## **Les régressions linéaires**

Nous souhaitons tout d'abord avoir une vue d'ensemble de l'évolution du PIB et celle de l'indice de confiance. L'indicateur synthètique représente cette indice de confiance. 
Nous réalisons donc un graphe pour voir globalement si les évolutions semblent conjointes.
"""

sns.set(rc = {'figure.figsize':(15,8)})
sns.lineplot(data=Data, x=Data.index, y='Indicateur synthétique', color="red", linewidth=2)
ax2=plt.twinx()
sns.lineplot(data=Data, x=Data.index, y='PIB', color="navy", linewidth=2, ax=ax2)
plt.title("Évolution de l'indicateur synthétique de confiance au cours du temps")

"""Nous pouvons ainsi attester que les évolutions semblent conjointes. Il est rare qu'une valeur augmente, sans qu'une autre n'augmente pas.

# *A. Régression linéaire du PIB sur l'indicateur de confiance.*

Nous réalisons tout d'abord la régression linéaire du PIB sur l'indicateur de confiance, appelé "indicateur synthétique" dans notre base. 
La sortie du code nous donne alors toutes les informations nécessaires sur le coefficient de corrélations. 
Cette sortie est alors davantage commentée dans le rapport joint.
"""

#On effectue la régression par la méthode des moindes carrés (OLS)

import statsmodels.api as sm

X = Data['Indicateur synthétique'].values
Y = Data['PIB'].values
X = X.astype(float)
Y = Y.astype(float)

x = sm.add_constant(X)
model = sm.OLS(Y, x)

results = model.fit()
# Avec  statsmodel, on a une sortie qui ressemble beaucoup à celle de Stata
print(results.summary())

#Le coefficient est positif, ce qui semble coherent, et on a un coefficient significatif à moins de 1%.

#On effectue un graphe avec Seaborn qui permet d'afficher la droite de régression sur le jeu de données.

Data = Data.astype(float)
sns.set(rc = {'figure.figsize':(15,8)})
sns.lmplot(x='Indicateur synthétique', y='PIB', data = Data)
plt.title("Régression linéaire du PIB nominal français sur l'indicateur synthétique de confiance")

"""# *B. Régression linéaire de la consommation des ménages sur l'indicateur de confiance*

On fait une seconde régression sur la dépense de consommation des ménages pour voir si la corrélation entre PIB et confiance est moins sensible que la corrélation entre confiance et dépense des ménages (cf théorie keynesienne).
"""

X = Data['Indicateur synthétique'].values
Y = Data['Consommation'].values
X = X.astype(int)
Y = Y.astype(int)

x = sm.add_constant(X)
model = sm.OLS(Y, x)
results = model.fit()
print(results.summary())

"""*On obtient un R² supérieur, ce qui indique une meilleure corrélation entre Consommation et indicateur de confiance, qu'entre PIB et indicateur de confiance.*

*Cela peut laisser à penser que, conformément à la théorie de Keynes, la confiance des ménages en l'avenir, leurs anticipations de la situation future, est une cause déterminante de l'évolution du PIB par le biais de la propension des ménages à dépenser (évidemment, nous n'avons pas ici tous les outils en main pour l'affirmer de manière définitive).*

---



## **Création de l'ACP (pour former un nouvel indicateur)**




---

Tout d'abord, nous reprenons la base de données formée des différents facteurs ayant servi à l'INSEE pour produire son indicateur synthétique, cad son indicateur de confiance. 

Pour réaliser une ACP, il nous faut un même nombre de colonnes et de lignes. 
Pour ce faire, on ne réalise qu'une ACP sur les 10 dernières années. 

Ces 10 dernières années sont assez révélatrices des changements en France. En effet : 
Les années 2011-2014 sont encore empreinte de la crise des subprimes, la confiance est toujours assez basse, avec une difficile reprise du PIB. 
De 2014 à 2020, la reprise économique semble possible, la confiance regagne les Français. 
La crise du Covid touche 2020 et 2021, avec une baisse du PIB et de la confiance.

# **I. ACP des observations (années)**

### *A. Préparation de notre table de donnée*
"""

indexNames = df[df.index < 2011.0].index #on crée indexNames comprenant toutes les lignes des données inférieures à 2011
df=df.drop(indexNames, axis=0) #on supprime ces lignes-ci 
del df['Indicateur synthétique'] #on supprime également la colonne "indicateur synthétique", qui est l'indicateur de confiance fourni par l'INSEE. On effet,
#le but de notre code est de créer un nouvel indicateur (en fonction des différents facteurs de la confiance)
df

"""On crée une nouvelle base X, à partir de df """

X = df

"""On vérifie ensuite le nombre de nos observations (n) et le nombre de nos facteurs (p) """

n,p = X.shape
print(n)
print(p)

"""# *B. Préparation de l'ACP*

On s'occupe tout d'abord de centrer et de réduire les variables (indispensable lors d'une ACP). 
Pour ce faire, on instancie l'objet. 

L'instanciation permet de créer des objets à partir d'une classe. 
Ici notre classe est StandardScaler. 
On peut créer alors notre nouvel objet Z, à partir de cette classe StandardScaler. Grâce à ça, Z est normalisée. On transforme donc X à partir de StandardScaler, c'est-à-dire grâce à la création d'un nouvel objet qui sera avec des variables centrées et réduites.
"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

Z = sc.fit_transform(X)

print(Z)

"""On vérifie ici simplement nos nouvelles données. Il faut qu'elles correspondent à des variables centrées réduites. """

print(np.mean(Z,axis=0))
#on vérifie ici que nos moyennes sont nulles, ce qui est le cas.

print(np.std(Z,axis=0,ddof=0))
#on vérifie ici que les écarts types sont bien égaux à 1.

"""# *C. Mise en oeuvre de l'ACP*

On importe une nouvelle classe PCA. Grâce à l'instanciation, on peut désormais créer un nouvel objet via cette classe, acp. acp a donc des paramètres selon ACP.
"""

from sklearn.decomposition import PCA
acp = PCA(svd_solver='full')

"""On crée une variable "coord". Celle-ci correspond alors aux cordonnées factorielles de Z, que l'on obtient grâce à la fonction fit_transform. 

Les coordonnées factorielles sont simplement définies comme les coordonnées de notre variable selon nos deux nouveaux axes. Ceci représente donc la corrélation entre une variable et les axes factoriels. 
"""

coord = acp.fit_transform(Z)

print(acp.n_components_)
#on vérifie que l'on a bien 11 composantes, c'est-à-dire p composantes (le nombre de facteurs)

"""Pour trouver les valeurs propres de notre nouvelle matrice, on utilise explained_variance, en corrigeant ces valeurs par n-1/n """

eigval = (n-1)/n*acp.explained_variance_ 
print(eigval)

"""On peut alors réaliser un "trace d'éboulis de valeurs propres", cad représenter les valeurs propres des facteurs de notre ACP. """

plt.plot(np.arange(1,p+1),eigval) 
plt.title("Trace d'éboulis") 
plt.ylabel("Valeur propre") 
plt.xlabel("Nombre de facteurs") 
plt.show()

"""On s'assure que l'on a un ratio de 1 grâce au graphique de la variance expliquée selon le nombre de facteurs. """

plt.plot(np.arange(1,p+1),np.cumsum(acp.explained_variance_ratio_)) 
plt.title("Variance expliquée selon le nombre de facteurs")
plt.ylabel("Ratio de variance expliqué par cumul")
plt.xlabel("Nombre de facteurs")
plt.show()
#ici on voit bien que le ratio est égal à 1 lorsqu'on a le nombre maximum de facteurs.

"""Pour interpréter l'ACP, on réalise le test des "bâtons brisés". On compare de ce fait la valeur des valeurs propres des facteurs à un certain seuil, ici calculé "S". Si la valeur propre est supérieure au seuil, le facteur est valide. """

S = 1/np.arange(p,0,-1) 
S = np.cumsum(S)
S = S[::-1]

Val_factors=[eigval[k]>=S[k] for k in range(len(S))]
print(pd.DataFrame({'Val.Propre':eigval,'Seuils':S,'Validité du facteur':Val_factors}))

"""Les deux premiers facteurs sont donc valides. 
Ceci confirme également ce qu'on aurait pu deviner grâce aux nombres de cassures de notre première figure. 

On avait une "cassure" à partir du deuxième facteur. Le nombre de facteur à retenir est donc de 2.

# *D. Représentation des observations (années) selon deux axes.*
"""

#positionnement des observations(=années) dans le premier plan
fig, axes = plt.subplots(figsize=(12,12)) #choix de la taille du graphique 
axes.set_xlim(-11,11) #même limites en abscisse. On choisit cette limite selon le nombre d'observations. 
axes.set_ylim(-11,11) #et en ordonnée
#placement des observations dans le plan 
for i in range(n): 
  plt.annotate(X.index[i],(coord[i,0],coord[i,1])) #on place selon leurs coordonnées calculées précédemment. 
#on définit les axes. 
plt.plot([-11,11],[0,0],color='silver',linestyle='-',linewidth=1) 
plt.plot([0,0],[-11,11],color='silver',linestyle='-',linewidth=1)
#affichage
plt.show()

"""La dispersion de nos observations semblent être logiques :  les années de faible confiance (2021 et 2020) sont presque orthogonales avec les années de forte confiance (entre 2015 et 2019). 
On retrouve, selon le premier axe, des années avec une confiance mitigée, entre 2011 et 2014.

# *E. Calcul de la contribution des observations aux deux axes*

On calcule le carré de la distance à l'origine de point. Ceci correspond à la contribution des observations dans l'inertie totale. 
On regarde plus exactement la dispersion des observations (années) par rapport à l'origine.
"""

#contribution des années dans l'inertie totale
di = np.sum(Z**2,axis=1) #on calcule la distance par rapport à l'origine 
print(pd.DataFrame({'ID':X.index,'d_i':di})) #on affiche la distance de chaque observation

"""A la suite de cette sortie, on peut analyser que les années "2011", "2013", "2017" et "2021" sont les deux années qui ont le plus contribué à l'inertie totale. On les retrouve aux extrémités du premier axe.

On peut alors calculer le cosinus carré, c'est-à-dire la qualité de la représentation.
"""

cos2 = coord**2 #on prend la coordonnée au carré 
for j in range(p): #p représente le nombre de facteurs 
  cos2[:,j] = cos2[:,j]/di #pour chaque observation, on divise ses coordonnées au carré (par exemple, toutes les lignes de la jième colonne) par le carré des distances à l'origine des observations. 
print(pd.DataFrame({'id':X.index,'COS2_1':cos2[:,0],'COS2_2':cos2[:,1]})) #on observe alors selon le premier et le deuxième axe

"""On vérifie la théorie - cad que la somme en ligne des cosinus carré est égale à 1 """

print(np.sum(cos2,axis=1))

"""Désormais, on peut calculer la contribution des observations aux axes. Grâce à ça, on peut déterminer quelles observations contribuent davantage à la définition de chaque facteur. """

ctr = coord**2 # on prend les coordonnées au carré 
for j in range(p): #p représente le nombre de facteurs
  ctr[:,j] = ctr[:,j]/(n*eigval[j]) #pour chaque observation, on divise les coordonnées au carré des observations (toutes les lignes de la jième colonne) par n x la valeur propre de l'observation. 
  print(pd.DataFrame({'id':X.index,'CTR_1':ctr[:,0],'CTR_2':ctr[:,1]})) #on peut alors observer selon le premier et le deuxième axe.

"""On a donc 2017, 2012, 2013, 2016 qui sont déterminants pour le premier axe. 
Pour le second, nous avons surtout 2011, 2020 et 2021. 

"""

#vérifions la théorie
print(np.sum(ctr,axis=0))
#théorie vérifiée, la somme est bien égale à 1.

"""# **II. Représentation des facteurs de la confiance**

# *A. Préparation des variables*

On calcule les vecteurs propres pour ensuite réaliser l'ACP des facteurs. En effet, il faut désormais calculer la matrice des corrélations entre les variables (observations) et les facteurs.
"""

print(acp.components_) #components permet le calcul des vecteurs propres
#racine carrée des valeurs propres
sqrt_eigval = np.sqrt(eigval)

#corrélation des variables avec les axes
corvar = np.zeros((p,p)) #on crée une matrice pxp (nombre de facteurs de confiance), formée avec des 0 
for k in range(p):
  corvar[:,k] = acp.components_[k,:] * sqrt_eigval[k] #à la ième ligne, on rajoute pour chaque colonne (allant de 1 à p) la valeur propre liée au facteur 
  # à laquelle on multiplie la valeur propre au carré 
#On affiche la matrice des corrélations variables x facteurs
print(corvar)
#on a, grâce à ce changement, les observations (années) en ligne, et les facteurs (de la confiance) en colonne.

"""Comme expliqué précédemment (lors du calcul pour les observations), on ne prend en compte que 2 facteurs (qui ont leur valeur propre supérieure au seuil). On affiche alors de ce fait que les deux premiers axes, c'est-à-dire les deux premières colonnes. """

#on affiche pour les deux premiers axes
print(pd.DataFrame({'id':X.columns,'COR_1':corvar[:,0],'COR_2':corvar[:,1]})) #on affiche que l'axe 0 et l'axe 1, cad la colonne 0 et la colonne 1

"""## *B. Cercle des corrélations*"""

#cercle des corrélations
fig, axes = plt.subplots(figsize=(8,8)) 
axes.set_xlim(-1,1) 
axes.set_ylim(-1,1)
#affichage des étiquettes (noms des variables)
for j in range(p): 
  plt.annotate(X.columns[j],(corvar[j,0],corvar[j,1])) #on place chaque facteur en fonction de leur valeur dans les deux premières colonnes de la matrice de covariance. 
#ajouter les axes
plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1) 
plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)

#ajouter un cercle
cercle = plt.Circle((0,0),1,color='blue',fill=False) 
axes.add_artist(cercle)
#affichage
plt.show()

""""Opportunité de faires des achats importants", "niveau de vie en France - perspective d'évolution" et "Niveau de vie en France - évolutions passées" sont très proches du cercle de corrélation et donc bien représentés sur le mapping. Ils sont de plus très proches de l'axe 1, indiquant qu'ils représentent très bien cet axe. 
Entre ces variables citées et "capacité d'épargne actuelle", "capacité d'épargne future", nous avons presque un angle droit. Les deux types de variable, niveau de vie et capacité d'épargne, semblent donc indépendantes.
Les capacités d'épargnes sont proches de l'axe 2, elles représentent donc très certainement bien cet axe.

# *C. Contribution des facteurs aux axes*

Tout comme notre premier graphique, on calcule alors la qualité des représentations de nos facteurs et leur contribution aux axes.
"""

#calcul de la qualité des représentations (comme en I. )
#cosinus carré des variables
cos2var = corvar**2 
print(pd.DataFrame({'id':X.columns,'COS2_1':cos2var[:,0],'COS2_2':cos2var[:,1]}))

#contributions aux axes (comme en I. )
ctrvar = cos2var
for k in range(p):
  ctrvar[:,k] = ctrvar[:,k]/eigval[k]
#on n'affiche que pour les deux premiers axes, i.e. les deux premières colonnes. 
print(pd.DataFrame({'id':X.columns,'CTR_1':ctrvar[:,0],'CTR_2':ctrvar[:,1]}))

"""Les niveaux de vie en France (passés et les perspectives)contribuent donc fortement à l'axe 1, ainsi que l'opportunité de faire des achats importants. 
Les capacités d'épargne (actuelle et futures), ainsi que les situaitons finanicères (personnelles et futures), contribuent fortement à l'axe 2. 
"""

